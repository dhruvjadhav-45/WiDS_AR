{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python mediapipe numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cuDYZbGRLp9N",
        "outputId": "bc5960fd-697b-4622-f4eb-69fd7276dda6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py~=2.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.3.1)\n",
            "Requirement already satisfied: sounddevice~=0.5 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice~=0.5->mediapipe) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Import the new API components for MediaPipe Tasks\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python.vision import FaceLandmarker, FaceLandmarkerOptions, RunningMode\n",
        "\n",
        "# Download the model asset for FaceLandmarker\n",
        "# This model is the successor to the older Face Mesh.\n",
        "# More details: https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python\n",
        "model_path = 'face_landmarker_with_attention.task'\n",
        "# Use !wget to download the model quietly (-q) and output to a specified file (-O).\n",
        "!wget -q -O {model_path} https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "\n",
        "# -----------------------------\n",
        "# Load video & overlay image\n",
        "# -----------------------------\n",
        "video_path = \"input_video.mp4\"\n",
        "overlay_path = \"overlay.png\"   # Ensure this is a transparent PNG\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "overlay_img = cv2.imread(overlay_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "# Validate overlay image to ensure it's a transparent PNG\n",
        "if overlay_img is None or overlay_img.shape[2] != 4:\n",
        "    raise ValueError(\"Overlay image must be a transparent PNG with an alpha channel\")\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "if fps == 0: # Handle cases where FPS might be 0, set a default\n",
        "    fps = 30.0 # Use float for calculations\n",
        "\n",
        "out = cv2.VideoWriter(\n",
        "    \"output_video.mp4\",\n",
        "    cv2.VideoWriter_fourcc(*\"mp4v\"), # Codec for output video\n",
        "    fps,\n",
        "    (width, height)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# MediaPipe Face Landmarker (New API Setup)\n",
        "# -----------------------------\n",
        "base_options = python.BaseOptions(model_asset_path=model_path)\n",
        "options = FaceLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    running_mode=RunningMode.VIDEO, # Set to VIDEO mode for processing video frames\n",
        "    num_faces=1, # Detect a single face\n",
        "    output_face_blendshapes=False, # Not needed for this task\n",
        "    output_facial_transformation_matrixes=False # Not needed for this task\n",
        ")\n",
        "# Create a FaceLandmarker object to detect faces and their landmarks.\n",
        "detector = FaceLandmarker.create_from_options(options)\n",
        "\n",
        "# -----------------------------\n",
        "# Overlay function (alpha blend with robust boundary handling)\n",
        "# -----------------------------\n",
        "def overlay_transparent(bg, overlay, x, y, w, h):\n",
        "    # Ensure overlay dimensions are positive\n",
        "    if w <= 0 or h <= 0:\n",
        "        return bg\n",
        "\n",
        "    overlay_resized = cv2.resize(overlay, (w, h))\n",
        "\n",
        "    # If overlay image does not have an alpha channel, return background as is\n",
        "    if overlay_resized.shape[2] < 4:\n",
        "        return bg\n",
        "\n",
        "    overlay_rgb = overlay_resized[:, :, :3]\n",
        "    alpha = overlay_resized[:, :, 3] / 255.0  # Alpha channel (0-1 range)\n",
        "\n",
        "    # Calculate valid region for overlay within background (clamping coordinates)\n",
        "    y1, y2 = max(0, y), min(bg.shape[0], y + h)\n",
        "    x1, x2 = max(0, x), min(bg.shape[1], x + w)\n",
        "\n",
        "    # Calculate corresponding region in overlay and mask based on clamped coordinates\n",
        "    overlay_crop_y1 = y1 - y\n",
        "    overlay_crop_y2 = h - (y + h - y2)\n",
        "    overlay_crop_x1 = x1 - x\n",
        "    overlay_crop_x2 = w - (x + w - x2)\n",
        "\n",
        "    # Extract the cropped overlay and mask, ensuring they are valid\n",
        "    if overlay_crop_y1 >= overlay_crop_y2 or overlay_crop_x1 >= overlay_crop_x2:\n",
        "        return bg # No valid area to overlay\n",
        "\n",
        "    overlay_cropped_rgb = overlay_rgb[overlay_crop_y1:overlay_crop_y2, overlay_crop_x1:overlay_crop_x2]\n",
        "    alpha_cropped = alpha[overlay_crop_y1:overlay_crop_y2, overlay_crop_x1:overlay_crop_x2]\n",
        "\n",
        "    # Perform alpha blending for each color channel\n",
        "    for c in range(3):\n",
        "        bg[y1:y2, x1:x2, c] = (\n",
        "            alpha_cropped * overlay_cropped_rgb[:, :, c] +\n",
        "            (1 - alpha_cropped) * bg[y1:y2, x1:x2, c]\n",
        "        )\n",
        "    return bg\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Smoothing variables for stable overlay placement\n",
        "# -----------------------------\n",
        "prev_cx, prev_cy, prev_eye_dist = None, None, None\n",
        "alpha_smooth = 0.7 # Smoothing factor (0.0 to 1.0; higher means more smoothing)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Process video frame by frame\n",
        "# -----------------------------\n",
        "timestamp_ms = 0 # Initialize timestamp for video mode processing\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: # Break if no frame is returned (end of video or error)\n",
        "        break\n",
        "\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert BGR to RGB for MediaPipe\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "    # Detect face landmarks using the new API\n",
        "    # The timestamp is crucial for `RunningMode.VIDEO`\n",
        "    face_landmarker_result = detector.detect_for_video(mp_image, timestamp_ms)\n",
        "\n",
        "    if face_landmarker_result.face_landmarks: # Check if any faces were detected\n",
        "        lm = face_landmarker_result.face_landmarks[0] # Get landmarks for the first detected face\n",
        "\n",
        "        # Use specific landmark indices for eye points from the 478-point mesh.\n",
        "        # These indices are commonly used in the FaceLandmarker model.\n",
        "        left_eye_landmark = lm[33]  # Approximate left eye outer corner\n",
        "        right_eye_landmark = lm[263] # Approximate right eye outer corner\n",
        "\n",
        "        # Convert normalized coordinates (0.0 to 1.0) to pixel coordinates\n",
        "        lx, ly = int(left_eye_landmark.x * width), int(left_eye_landmark.y * height)\n",
        "        rx, ry = int(right_eye_landmark.x * width), int(right_eye_landmark.y * height)\n",
        "\n",
        "        # Calculate center point between the eyes for overlay placement\n",
        "        cx = (lx + rx) // 2\n",
        "        cy = (ly + ry) // 2\n",
        "\n",
        "        # Calculate distance between eyes for scaling the overlay\n",
        "        eye_dist = int(np.hypot(rx - lx, ry - ly))\n",
        "\n",
        "        # Apply smoothing to the position and size to make the overlay movement less jittery\n",
        "        if prev_cx is None:\n",
        "            # Initialize smoothing variables on the first detected frame\n",
        "            prev_cx, prev_cy, prev_eye_dist = cx, cy, eye_dist\n",
        "        else:\n",
        "            # Apply exponential moving average for smoothing\n",
        "            cx = int(alpha_smooth * prev_cx + (1 - alpha_smooth) * cx)\n",
        "            cy = int(alpha_smooth * prev_cy + (1 - alpha_smooth) * cy)\n",
        "            eye_dist = int(alpha_smooth * prev_eye_dist + (1 - alpha_smooth) * eye_dist)\n",
        "\n",
        "            # Update previous values for the next frame\n",
        "            prev_cx, prev_cy, prev_eye_dist = cx, cy, eye_dist\n",
        "\n",
        "        # Determine glasses size based on eye distance and aspect ratio of overlay image\n",
        "        overlay_w = int(2.0 * eye_dist) # Scale factor for width, adjust as needed\n",
        "        overlay_h = int(\n",
        "            overlay_w * overlay_img.shape[0] / overlay_img.shape[1] # Maintain aspect ratio\n",
        "        )\n",
        "\n",
        "        # Calculate position for the overlay (centered between eyes, slightly lifted)\n",
        "        x = cx - overlay_w // 2\n",
        "        y = cy - overlay_h // 2 - int(0.15 * overlay_h) # Adjust Y to lift glasses slightly above eyes\n",
        "\n",
        "        # Clamp overlay position within frame boundaries to prevent drawing out of bounds\n",
        "        x = max(0, min(x, width - overlay_w))\n",
        "        y = max(0, min(y, height - overlay_h))\n",
        "\n",
        "        # Apply the transparent overlay to the frame\n",
        "        frame = overlay_transparent(\n",
        "            frame, overlay_img, x, y, overlay_w, overlay_h\n",
        "        )\n",
        "\n",
        "    out.write(frame) # Write the processed frame to the output video\n",
        "    # cv2.imshow(\"Glasses Overlay\", frame) # cv2.imshow does not work in Colab environments directly\n",
        "\n",
        "    # if cv2.waitKey(1) & 0xFF == 27: # This key press listener also won't work without a display\n",
        "    #     break\n",
        "\n",
        "    timestamp_ms += int(1000 / fps) # Increment timestamp for the next frame processing\n",
        "\n",
        "# -----------------------------\n",
        "# Cleanup\n",
        "# -----------------------------\n",
        "cap.release() # Release video capture object\n",
        "out.release() # Release video writer object\n",
        "# cv2.destroyAllWindows() # Not applicable in Colab as there's no display\n",
        "detector.close() # Close the MediaPipe FaceLandmarker detector to release resources"
      ],
      "metadata": {
        "id": "NOlEq4hDoRHD"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}